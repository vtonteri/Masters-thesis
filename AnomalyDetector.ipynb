{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is part of Ville Tonteris master's thesis. \n",
    "\n",
    "\"Underwater sound anomaly detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all necessary libraries\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os\n",
    "import torch\n",
    "import torchaudio\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "import torchaudio.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundDataset(Dataset):\n",
    "    def __init__(self, annotations_file, audio_dir, transformation, target_sample_rate, num_samples, device):\n",
    "        self.annotations = pd.read_excel(annotations_file)\n",
    "        self.audio_dir = audio_dir\n",
    "        self.transformation = transformation\n",
    "        self.target_sample_rate = target_sample_rate\n",
    "        self.num_samples = num_samples\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        audio_sample_path = self._get_audio_sample_path(index)\n",
    "        label = self._get_audio_sample_label(index)\n",
    "        signal, sr = torchaudio.load(audio_sample_path)\n",
    "        signal = signal.to(self.device)\n",
    "        signal = self._resample_if_necessary(signal, sr)\n",
    "        signal = self._mix_down_if_necessary(signal)\n",
    "        signal = self._cut_if_necessary(signal)\n",
    "        signal = self._right_pad_if_necessary(signal)\n",
    "        signal = self.transformation(signal)\n",
    "        return signal, label\n",
    "\n",
    "    def _cut_if_necessary(self, signal):\n",
    "        if signal.shape[1] > self.num_samples:\n",
    "            signal = signal[:, :self.num_samples]\n",
    "        return signal\n",
    "\n",
    "    def _right_pad_if_necessary(self, signal):\n",
    "        length_signal = signal.shape[1]\n",
    "        if length_signal < self.num_samples:\n",
    "            num_missing_samples = self.num_samples - length_signal\n",
    "            last_dim_padding = (0, num_missing_samples)\n",
    "            signal = torch.nn.functional.pad(signal, last_dim_padding)\n",
    "        return signal\n",
    "\n",
    "    def _resample_if_necessary(self, signal, sr):\n",
    "        if sr != self.target_sample_rate:\n",
    "            resampler = torchaudio.transforms.Resample(sr, self.target_sample_rate).to(self.device)\n",
    "            signal = resampler(signal)\n",
    "        return signal\n",
    "\n",
    "    def _mix_down_if_necessary(self, signal):\n",
    "        if signal.shape[0] > 1:\n",
    "            signal = torch.mean(signal, dim=0, keepdim=True)\n",
    "        return signal\n",
    "\n",
    "    def _get_audio_sample_path(self, index):\n",
    "        filename = self.annotations.iloc[index]['Filename']\n",
    "        path = os.path.join(self.audio_dir, filename)\n",
    "        return path\n",
    "\n",
    "    def _get_audio_sample_label(self, index):\n",
    "        return self.annotations.iloc[index]['Type']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define neural network model\n",
    "\n",
    "class ClassificationModel:\n",
    "    \n",
    "    def __init__(self) -> None:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# Example: iterate over the dataset\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, targets \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m---> 23\u001b[0m     \u001b[38;5;28mprint\u001b[39m(inputs\u001b[38;5;241m.\u001b[39mshape, \u001b[43mtargets\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m)\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;66;03m# Your training code here\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "ANNOTATIONS_FILE = r'C:\\Users\\elliw\\OneDrive\\Documents\\Aalto\\Masters Thesis\\shipsEar_AUDIOS\\shipsEar.xlsx'\n",
    "AUDIO_DIR = r'C:\\Users\\elliw\\OneDrive\\Documents\\Aalto\\Masters Thesis\\shipsEar_AUDIOS'\n",
    "SAMPLE_RATE = 16000\n",
    "NUM_SAMPLES = 16000\n",
    "\n",
    "# Define transformations\n",
    "transformation = transforms.MelSpectrogram(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mels=64,\n",
    "    n_fft=1024,\n",
    "    hop_length=512\n",
    ")\n",
    "\n",
    "# Initialize dataset\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "dataset = SoundDataset(ANNOTATIONS_FILE, AUDIO_DIR, transformation, SAMPLE_RATE, NUM_SAMPLES, device)\n",
    "\n",
    "# Create dataloader\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "model = ClassificationModel()\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    print(inputs[0].shape, targets)\n",
    "    # Your training code here\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
